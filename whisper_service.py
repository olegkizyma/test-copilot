#!/usr/bin/env python3
"""
ATLAS Whisper Speech Recognition Service
–°–µ—Ä–≤—ñ—Å –¥–ª—è —Ä–æ–∑–ø—ñ–∑–Ω–∞–≤–∞–Ω–Ω—è –º–æ–≤–∏ –∑ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è–º faster-whisper Large v3
"""

import os
import io
import logging
import tempfile
from datetime import datetime
from pathlib import Path
from flask import Flask, request, jsonify
from flask_cors import CORS
from faster_whisper import WhisperModel

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(name)s: %(message)s'
)
logger = logging.getLogger('atlas.whisper')

# –ö–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—è
WHISPER_PORT = int(os.environ.get('WHISPER_PORT', 3002))
WHISPER_MODEL = 'medium'  # –ó–º—ñ–Ω—é—î–º–æ –Ω–∞ –±—ñ–ª—å—à —Å—Ç–∞–±—ñ–ª—å–Ω—É –º–æ–¥–µ–ª—å
DEVICE = "auto"  # faster-whisper –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ –æ–±–µ—Ä–µ –Ω–∞–π–∫—Ä–∞—â–∏–π –ø—Ä–∏—Å—Ç—Ä—ñ–π
COMPUTE_TYPE = "float32"  # –î–ª—è —Å—Ç–∞–±—ñ–ª—å–Ω–æ—Å—Ç—ñ –Ω–∞ Apple Silicon

# –°—Ç–≤–æ—Ä–µ–Ω–Ω—è Flask app
app = Flask(__name__)
CORS(app)

# –ì–ª–æ–±–∞–ª—å–Ω—ñ –∑–º—ñ–Ω–Ω—ñ –¥–ª—è –º–æ–¥–µ–ª—ñ
whisper_model = None

def load_whisper_model():
    """–ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ faster-whisper Large v3"""
    global whisper_model
    
    if whisper_model is not None:
        return whisper_model
    
    logger.info(f"ü§ñ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è faster-whisper {WHISPER_MODEL} –º–æ–¥–µ–ª—ñ...")
    logger.info(f"Device: {DEVICE}, Compute type: {COMPUTE_TYPE}")
    start_time = datetime.now()
    
    try:
        # –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ Large v3 –º–æ–¥–µ–ª—å
        whisper_model = WhisperModel(
            WHISPER_MODEL,
            device=DEVICE,
            compute_type=COMPUTE_TYPE,
            download_root=None,  # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—É –¥–∏—Ä–µ–∫—Ç–æ—Ä—ñ—é
            local_files_only=False
        )
        
        load_time = (datetime.now() - start_time).total_seconds()
        logger.info(f"‚úÖ faster-whisper {WHISPER_MODEL} –º–æ–¥–µ–ª—å –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–∞ —É—Å–ø—ñ—à–Ω–æ –∑–∞ {load_time:.2f} —Å–µ–∫—É–Ω–¥!")
        
        return whisper_model
        
    except Exception as e:
        logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ: {e}")
        
        # Fallback –¥–æ CPU —è–∫—â–æ –Ω–µ –≤–¥–∞–ª–æ—Å—è
        try:
            logger.info("–°–ø—Ä–æ–±–∞ fallback –Ω–∞ CPU...")
            whisper_model = WhisperModel(
                WHISPER_MODEL,
                device="cpu",
                compute_type="float32"
            )
            load_time = (datetime.now() - start_time).total_seconds()
            logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–∞ –Ω–∞ CPU –∑–∞ {load_time:.2f} —Å–µ–∫—É–Ω–¥")
            return whisper_model
            
        except Exception as cpu_e:
            logger.error(f"‚ùå CPU fallback —Ç–∞–∫–æ–∂ –Ω–µ –≤–¥–∞–≤—Å—è: {cpu_e}")
            return None

@app.route('/health')
def health():
    """–ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ —Å—Ç–∞–Ω—É —Å–µ—Ä–≤—ñ—Å—É"""
    global whisper_model
    
    try:
        model_loaded = whisper_model is not None
        return jsonify({
            'status': 'ok',
            'model_loaded': model_loaded,
            'model_name': WHISPER_MODEL if model_loaded else None,
            'device': DEVICE,
            'compute_type': COMPUTE_TYPE,
            'timestamp': datetime.now().isoformat(),
            'service': 'atlas-whisper-large-v3'
        })
    except Exception as e:
        return jsonify({
            'status': 'error',
            'error': str(e),
            'timestamp': datetime.now().isoformat()
        }), 500

@app.route('/transcribe', methods=['POST'])
def transcribe_audio():
    """
    –†–æ–∑–ø—ñ–∑–Ω–∞–≤–∞–Ω–Ω—è –º–æ–≤–∏ –∑ –∞—É–¥—ñ–æ —Ñ–∞–π–ª—É
    
    Expected:
    - audio file in request.files['audio']
    - optional: language (uk, en, etc.)
    
    Returns:
    - JSON with transcribed text
    """
    try:
        # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ –Ω–∞—è–≤–Ω—ñ—Å—Ç—å –∞—É–¥—ñ–æ —Ñ–∞–π–ª—É
        if 'audio' not in request.files:
            return jsonify({
                'error': 'No audio file provided',
                'status': 'error'
            }), 400
        
        audio_file = request.files['audio']
        if audio_file.filename == '':
            return jsonify({
                'error': 'Empty audio file',
                'status': 'error'
            }), 400
        
        # –û—Ç—Ä–∏–º—É—î–º–æ –º–æ–≤—É (–∑–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º —É–∫—Ä–∞—ó–Ω—Å—å–∫–∞)
        language = request.form.get('language', 'uk')
        
        logger.info(f"üé§ Transcribing audio file: {audio_file.filename}, language: {language}")
        
        # –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ –º–æ–¥–µ–ª—å —è–∫—â–æ –Ω–µ–æ–±—Ö—ñ–¥–Ω–æ
        model = load_whisper_model()
        if model is None:
            return jsonify({
                'error': 'Whisper model not available',
                'status': 'error'
            }), 500
        
        # –î–æ–¥–∞—Ç–∫–æ–≤—ñ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü—ñ—ó
        beam_size = int(request.form.get('beam_size', 5))
        word_timestamps = request.form.get('word_timestamps', 'false').lower() == 'true'
        use_vad = request.form.get('use_vad', 'true').lower() == 'true'
        
        logger.info(f"–ü–∞—Ä–∞–º–µ—Ç—Ä–∏: language={language}, beam_size={beam_size}, word_timestamps={word_timestamps}, use_vad={use_vad}")
        
        # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ –∞—É–¥—ñ–æ —É —Ç–∏–º—á–∞—Å–æ–≤–∏–π —Ñ–∞–π–ª
        with tempfile.NamedTemporaryFile(delete=False, suffix='.wav') as temp_file:
            audio_file.save(temp_file.name)
            temp_path = temp_file.name
        
        try:
            # –†–æ–∑–ø—ñ–∑–Ω–∞—î–º–æ –º–æ–≤—É –∑ Large v3
            start_time = datetime.now()
            
            # –ù–∞–ª–∞—à—Ç–æ–≤—É—î–º–æ VAD –ø–∞—Ä–∞–º–µ—Ç—Ä–∏
            transcribe_params = {
                'beam_size': beam_size,
                'language': language if language != 'auto' else None,
                'word_timestamps': word_timestamps,
                'vad_filter': use_vad
            }
            
            if use_vad:
                transcribe_params['vad_parameters'] = dict(
                    min_silence_duration_ms=2000,  # –ó–±—ñ–ª—å—à—É—î–º–æ –º—ñ–Ω—ñ–º–∞–ª—å–Ω—É —Ç—Ä–∏–≤–∞–ª—ñ—Å—Ç—å –º–æ–≤—á–∞–Ω–Ω—è
                    threshold=0.3,  # –ó–Ω–∏–∂—É—î–º–æ –ø–æ—Ä—ñ–≥ –¥–µ—Ç–µ–∫—Ü—ñ—ó –≥–æ–ª–æ—Å—É –¥–ª—è –±—ñ–ª—å—à–æ—ó —á—É—Ç–ª–∏–≤–æ—Å—Ç—ñ
                    min_speech_duration_ms=100  # –ú—ñ–Ω—ñ–º–∞–ª—å–Ω–∞ —Ç—Ä–∏–≤–∞–ª—ñ—Å—Ç—å –º–æ–≤–∏
                )
            
            segments, info = model.transcribe(temp_path, **transcribe_params)
            
            # –ó–±–∏—Ä–∞—î–º–æ —Ç–µ–∫—Å—Ç –∑ —É—Å—ñ—Ö —Å–µ–≥–º–µ–Ω—Ç—ñ–≤
            transcription_segments = []
            full_text_parts = []
            
            for segment in segments:
                segment_data = {
                    'start': segment.start,
                    'end': segment.end,
                    'text': segment.text
                }
                
                if word_timestamps and hasattr(segment, 'words'):
                    segment_data['words'] = [
                        {
                            'start': word.start,
                            'end': word.end,
                            'word': word.word,
                            'probability': word.probability
                        }
                        for word in segment.words
                    ]
                
                transcription_segments.append(segment_data)
                full_text_parts.append(segment.text)
            
            full_text = ' '.join(full_text_parts).strip()
            transcription_time = (datetime.now() - start_time).total_seconds()
            
            # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —á–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤–∞–ª—ñ–¥–Ω–∏–π
            if not is_valid_transcription(full_text, info.duration):
                logger.info(f"üö´ –†–µ–∑—É–ª—å—Ç–∞—Ç –≤—ñ–¥—Ñ—ñ–ª—å—Ç—Ä–æ–≤–∞–Ω–æ: '{full_text}'")
                return jsonify({
                    'status': 'filtered',
                    'text': '',
                    'reason': 'Suspicious or too short transcription',
                    'original_text': full_text,
                    'duration': round(info.duration, 2),
                    'transcription_time': round(transcription_time, 2),
                    'timestamp': datetime.now().isoformat()
                })
            
            logger.info(f"‚úÖ –¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü—ñ—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ {transcription_time:.2f}—Å: '{full_text[:100]}...'")
            
            response_data = {
                'status': 'success',
                'text': full_text,
                'language': info.language,
                'language_probability': round(info.language_probability, 4),
                'duration': round(info.duration, 2),
                'transcription_time': round(transcription_time, 2),
                'model': WHISPER_MODEL,
                'segments': transcription_segments if word_timestamps else None,
                'timestamp': datetime.now().isoformat()
            }
            
            return jsonify(response_data)
            
        finally:
            # –í–∏–¥–∞–ª—è—î–º–æ —Ç–∏–º—á–∞—Å–æ–≤–∏–π —Ñ–∞–π–ª
            try:
                os.unlink(temp_path)
            except Exception as cleanup_error:
                logger.warning(f"–ù–µ –≤–¥–∞–ª–æ—Å—è –≤–∏–¥–∞–ª–∏—Ç–∏ —Ç–∏–º—á–∞—Å–æ–≤–∏–π —Ñ–∞–π–ª: {cleanup_error}")
                
    except Exception as e:
        logger.error(f"‚ùå Transcription error: {e}")
        return jsonify({
            'error': str(e),
            'status': 'error'
        }), 500

@app.route('/transcribe_blob', methods=['POST'])
def transcribe_blob():
    """
    –†–æ–∑–ø—ñ–∑–Ω–∞–≤–∞–Ω–Ω—è –º–æ–≤–∏ –∑ –±—ñ–Ω–∞—Ä–Ω–∏—Ö –¥–∞–Ω–∏—Ö
    
    Expected:
    - binary audio data in request.data
    - optional: language in query params
    
    Returns:
    - JSON with transcribed text
    """
    try:
        # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ –Ω–∞—è–≤–Ω—ñ—Å—Ç—å –¥–∞–Ω–∏—Ö
        if not request.data:
            return jsonify({
                'error': 'No audio data provided',
                'status': 'error'
            }), 400
        
        # –û—Ç—Ä–∏–º—É—î–º–æ –º–æ–≤—É —Ç–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏
        language = request.args.get('language', 'uk')
        use_vad = request.args.get('use_vad', 'true').lower() == 'true'
        
        logger.info(f"üé§ Transcribing audio blob ({len(request.data)} bytes), language: {language}, use_vad: {use_vad}")
        
        # –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ –º–æ–¥–µ–ª—å —è–∫—â–æ –Ω–µ–æ–±—Ö—ñ–¥–Ω–æ
        model = load_whisper_model()
        if model is None:
            return jsonify({
                'error': 'Whisper model not available',
                'status': 'error'
            }), 500
        
        # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ –¥–∞–Ω—ñ —É —Ç–∏–º—á–∞—Å–æ–≤–∏–π —Ñ–∞–π–ª
        with tempfile.NamedTemporaryFile(delete=False, suffix='.wav') as temp_file:
            temp_file.write(request.data)
            temp_path = temp_file.name
        
        try:
            # –†–æ–∑–ø—ñ–∑–Ω–∞—î–º–æ –º–æ–≤—É
            start_time = datetime.now()
            
            # –ù–∞–ª–∞—à—Ç–æ–≤—É—î–º–æ VAD –ø–∞—Ä–∞–º–µ—Ç—Ä–∏
            transcribe_params = {
                'language': language if language != 'auto' else None,
                'vad_filter': use_vad
            }
            
            if use_vad:
                transcribe_params['vad_parameters'] = dict(
                    min_silence_duration_ms=2000,  # –ó–±—ñ–ª—å—à—É—î–º–æ –º—ñ–Ω—ñ–º–∞–ª—å–Ω—É —Ç—Ä–∏–≤–∞–ª—ñ—Å—Ç—å –º–æ–≤—á–∞–Ω–Ω—è
                    threshold=0.3,  # –ó–Ω–∏–∂—É—î–º–æ –ø–æ—Ä—ñ–≥ –¥–µ—Ç–µ–∫—Ü—ñ—ó –≥–æ–ª–æ—Å—É –¥–ª—è –±—ñ–ª—å—à–æ—ó —á—É—Ç–ª–∏–≤–æ—Å—Ç—ñ
                    min_speech_duration_ms=100  # –ú—ñ–Ω—ñ–º–∞–ª—å–Ω–∞ —Ç—Ä–∏–≤–∞–ª—ñ—Å—Ç—å –º–æ–≤–∏
                )
            
            segments, info = model.transcribe(temp_path, **transcribe_params)
            
            transcription_time = (datetime.now() - start_time).total_seconds()
            
            # –ó–±–∏—Ä–∞—î–º–æ —Ç–µ–∫—Å—Ç –∑ —É—Å—ñ—Ö —Å–µ–≥–º–µ–Ω—Ç—ñ–≤
            full_text_parts = []
            for segment in segments:
                full_text_parts.append(segment.text)
            
            text = ' '.join(full_text_parts).strip()
            detected_language = info.language
            
            # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —á–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤–∞–ª—ñ–¥–Ω–∏–π
            if not is_valid_transcription(text, info.duration):
                logger.info(f"üö´ Blob —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤—ñ–¥—Ñ—ñ–ª—å—Ç—Ä–æ–≤–∞–Ω–æ: '{text}'")
                return jsonify({
                    'status': 'filtered',
                    'text': '',
                    'reason': 'Suspicious or too short transcription',
                    'original_text': text,
                    'duration': round(info.duration, 2),
                    'transcription_time': transcription_time,
                    'model': WHISPER_MODEL,
                    'device': DEVICE
                })
            
            logger.info(f"‚úÖ Blob transcription completed in {transcription_time:.2f}s: '{text[:50]}...'")
            
            return jsonify({
                'status': 'success',
                'text': text,
                'language': detected_language,
                'transcription_time': transcription_time,
                'model': WHISPER_MODEL,
                'device': DEVICE
            })
            
        finally:
            # –í–∏–¥–∞–ª—è—î–º–æ —Ç–∏–º—á–∞—Å–æ–≤–∏–π —Ñ–∞–π–ª
            try:
                os.unlink(temp_path)
            except:
                pass
                
    except Exception as e:
        logger.error(f"‚ùå Blob transcription error: {e}")
        return jsonify({
            'error': str(e),
            'status': 'error'
        }), 500

@app.route('/models')
def list_models():
    """–°–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω–∏—Ö –º–æ–¥–µ–ª–µ–π Whisper"""
    available_models = [
        'tiny', 'tiny.en',
        'base', 'base.en', 
        'small', 'small.en',
        'medium', 'medium.en',
        'large-v1', 'large-v2', 'large-v3'
    ]
    
    return jsonify({
        'available_models': available_models,
        'current_model': WHISPER_MODEL,
        'device': DEVICE,
        'model_loaded': whisper_model is not None
    })

def initialize_service():
    """–Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è —Å–µ—Ä–≤—ñ—Å—É Whisper"""
    logger.info("üöÄ –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è ATLAS Whisper Large v3 Service...")
    logger.info(f"–ü–æ—Ä—Ç: {WHISPER_PORT}")
    logger.info(f"–ú–æ–¥–µ–ª—å: {WHISPER_MODEL}")
    logger.info(f"–ü—Ä–∏—Å—Ç—Ä—ñ–π: {DEVICE}")
    
    try:
        load_whisper_model()
        logger.info("‚úÖ –°–µ—Ä–≤—ñ—Å —ñ–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–æ–≤–∞–Ω–æ —É—Å–ø—ñ—à–Ω–æ!")
    except Exception as e:
        logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ —ñ–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—ó: {e}")
        raise

if __name__ == '__main__':
    try:
        initialize_service()
        
        logger.info(f"üåê –ó–∞–ø—É—Å–∫ —Å–µ—Ä–≤–µ—Ä–∞ –Ω–∞ –ø–æ—Ä—Ç—É {WHISPER_PORT}...")
        app.run(
            host='0.0.0.0',
            port=WHISPER_PORT,
            debug=False,
            threaded=True
        )
        
    except KeyboardInterrupt:
        logger.info("üõë –°–µ—Ä–≤—ñ—Å –∑—É–ø–∏–Ω–µ–Ω–æ –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–µ–º")
    except Exception as e:
        logger.error(f"‚ùå –ö—Ä–∏—Ç–∏—á–Ω–∞ –ø–æ–º–∏–ª–∫–∞: {e}")
        exit(1)